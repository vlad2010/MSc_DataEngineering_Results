Well I guess it's better than no parallelization regardless of how big the array is right?  So lets start, why don't you start writing the kernel for the... wait what's the kernel going to do?  We divide up the array by number of threads,  and each thread will check if its own middle is equal to the hash then if it is return a signal that the search is over, if not then return some default signal which could say lower or higher?  Then these messages need to be synchronized back in the host or some central place, assert that the  messages make logical sense and so if there are k subarrays at runtime  of O(k) need to find the parts of this message array that is like [...higher, lower,...]  which means the new low and high would be the middles of these two subsequent arrays right?  Ie. the only place the hash string being searched could be is between those.  But also consider CUDA kernels that use grid stride loops and then calculate how to assign a certain chunk of the array to threads and let them grid stride through it?  With grid striding, no matter how big the hash database and how small my number of threads with a weaker GPU, I'm guaranteed to get some parallelism wouldn't I?  Give me your honest thoughts on my thoughts here and which way you wan to take this.